<?xml version="1.0" encoding="UTF-8"?>
<section xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://docbook.org/ns/docbook" xml:id="clusterstcnf">
      <title>Cluster Installation and Configuration</title>
      <!-- ======================================== -->

      <section xml:id="clusterstcnf7">
        <title>Virtuoso Elastic Cluster Installation &amp; Configuration -- Version 7.x</title>
        <para>These sections apply to Virtuoso as of version 7.x .</para>
        <section xml:id="clusterstcnfconf7what">
          <title>What</title>
          <para>Virtuoso 7 and later versions can be configured in Elastic scale out cluster mode. The data is sharded in a
         	large number of self-contained partitions. These partitions are divided among a number of database server processes
         	and can migrate between them.</para>
        </section>
        <section xml:id="clusterstcnfconf7why">
          <title>Why</title>
          <para>An Elastic cluster can be dynamically resized with new partitions or shards dynamically added to the
         	same or new hardware resources as the need to increase the size of the scale cluster or relocation of
         	partitions is required.</para>
        </section>
        <section xml:id="clusterstcnfconf7how">
          <title>How</title>
          <para>This documentation details the steps for the installation and configuration of a Virtuoso Elastic Cluster on Unix:</para>
          <orderedlist>
            <listitem>
                <para><link linkend="clusterstcnfconf7unix">Perform Virtuoso 7 Unix installation</link></para>
            </listitem>
            <listitem>
              <para><link linkend="clusterstcnfconf7clsize">Determine Elastic Cluster size</link></para>
            </listitem>
            <listitem>
              <para><link linkend="clusterstcnfconf7enmode">Enable Elastic Cluster mode</link></para>
            </listitem>
            <listitem>
              <para><link linkend="clusterstcnfconf7start">Start Elastic Cluster</link></para>
            </listitem>
            <listitem>
              <para><link linkend="clusterstcnfconf7split">Splitting Cluster nodes across different machines.</link></para>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconf7unix">
          <title>Virtuoso 7 Unix installation</title>
          <para><emphasis>Step 1</emphasis>

  : Perform Virtuoso 7 Unix installation</para>
          <programlisting>
$ tar xvf virtuoso-universal-server-7.x.tar
x install.sh
x universal-server.taz
$ sh install.sh

- Extracting Virtuoso Universal Server v7.x

- Creating default environment settings

- Creating default database settings
Configuring: database
Creating directory $VIRTUOSO_HOME/database/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/database
Installing new php.ini in $VIRTUOSO_HOME/database
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t

- Registering ODBC drivers

- Registering .NET provider for Mono

- Installing VAD packages in database (this can take some time)

- Checking where license should be stored

- Starting OpenLink License Manager

- Checking for initial Virtuoso license

- Starting Virtuoso server instance

- Finalizing installation

This concludes the first part of the installation.

- Attempting to start browser

Please start a browser manually and open the following URL to finalize
the installation process:

     http://cname:8890/install/

Installation completed
$
</programlisting>
        </section>
        <section xml:id="clusterstcnfconf7clsize">
          <title>Determine Elastic Cluster size</title>
          <para><emphasis>Step 2</emphasis>

  : Determine Elastic Cluster size</para>
          <orderedlist>
            <listitem>
              <para>Determine how many nodes you want to start the elastic cluster with: 2, 3, 4, 5, 6, 7, 8 etc. Note:
           	the number of the nodes must be &gt;= 2 </para>
            </listitem>
            <listitem>
              <para>Setup Virtuoso environment and stop default database:
</para>
              <programlisting>
$ . ./virtuoso-environment.sh
$ virtuoso-stop.sh
Shutting down Virtuoso instance in [database]
$
</programlisting>
            </listitem>
            <listitem>
              <para>Optionally remove the default database such that is does not get started with the cluster:
</para>
              <programlisting>
rm  -rf database
</programlisting>
            </listitem>
            <listitem>
              <para>Full list of supported options for the script "virtuoso-mkcluster.sh":
</para>
              <programlisting>
  -cluster-size       Number of nodes in cluster
  -cluster-node       Node number in a cluster
  -cluster-port       Base portnumer for cluster
  -cluster-ipc-port   Base portnumer for cluster IPC
  -cluster_size       Size of the cluster
  -virtuoso_home      Virtuoso home path
</programlisting>
            </listitem>
            <listitem>
              <para>Run the virtuoso-mkcluster.sh script to create the cluster, note the default number of nodes is 4,
           	but this can be changed by setting the environment variable CLUSTER_SIZE to the required number of nodes to
           	be created before running the script:
</para>
              <programlisting>
$ virtuoso-mkcluster.sh
Configuring cluster node: 1/4

Creating directory $VIRTUOSO_HOME/cluster_01
Creating directory $VIRTUOSO_HOME/cluster_01/backup
Creating directory $VIRTUOSO_HOME/cluster_01/logs
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_01
Installing new php.ini in $VIRTUOSO_HOME/cluster_01
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 2/4

Creating directory $VIRTUOSO_HOME/cluster_02
Creating directory $VIRTUOSO_HOME/cluster_02/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_02
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 3/4

Creating directory $VIRTUOSO_HOME/cluster_03
Creating directory $VIRTUOSO_HOME/cluster_03/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_03
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 4/4

Creating directory $VIRTUOSO_HOME/cluster_04
Creating directory $VIRTUOSO_HOME/cluster_04/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_04
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
$
</programlisting>
            </listitem>
            <listitem>
              <para>In each cluster_XX directory the following the
              <code>cluster.ini</code>
               files are created for cluster
           internal communication:
</para>
              <programlisting>
$ more cluster_01/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host1
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = localhost:22201
Host2               = localhost:22202
Host3               = localhost:22203
Host4               = localhost:22204
MaxHosts            = 5
$ more cluster_02/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host2
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = localhost:22201
Host2               = localhost:22202
Host3               = localhost:22203
Host4               = localhost:22204
MaxHosts            = 5
$ more cluster_03/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host3
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = localhost:22201
Host2               = localhost:22202
Host3               = localhost:22203
Host4               = localhost:22204
MaxHosts            = 5
$ more cluster_04/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host4
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = localhost:22201
Host2               = localhost:22202
Host3               = localhost:22203
Host4               = localhost:22204
MaxHosts            = 5
$
</programlisting>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconf7enmode">
          <title>Enable Elastic Cluster mode</title>
          <para><emphasis>Step 3</emphasis>

  : Enable Elastic Cluster mode</para>
          <orderedlist>
            <listitem>
              <para>The <code>cluster.ini</code> files need to be reconfigured as detailed below for elastic cluster and
           file slicing/sharding to be enabled.</para>
            </listitem>
            <listitem>
              <para>A common file called <code>cluster.global.ini</code> can be created and placed in the home directory
           of the Virtuoso installation:
</para>
              <programlisting>
[Cluster]
Threads             = 300
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 100
RDFLoadBytes        = 52428800
Host1               = localhost:22201
Host2               = localhost:22202
Host3               = localhost:22203
Host4               = localhost:22204
</programlisting>
            </listitem>
            <listitem>
              <para>This file can then be sym-linked to each of the <code>cluster_XX</code> directories of the cluster,
           and its content will be merged with the <code>cluster.ini</code> in the respective directory of each node
           when starting the cluster.
</para>
              <programlisting>
ln -s cluster.global.ini cluster_01/cluster.global.ini
ln -s cluster.global.ini cluster_02/cluster.global.ini
ln -s cluster.global.ini cluster_03/cluster.global.ini
ln -s cluster.global.ini cluster_04/cluster.global.ini
</programlisting>
            </listitem>
            <listitem>
              <para>Edit each of the the <code>cluster.ini</code> files in the <code>cluster_XX</code> directories as
           follows to enable, elastic cluster mode:
</para>
              <programlisting>
$ more cluster_01/cluster.ini
[Cluster]
Master              = Host1
ThisHost            = Host1

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 6
MaxSlices = 2048

$ more cluster_02/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host2

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 6
MaxSlices = 2048

$ more cluster_03/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host3

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 6
MaxSlices = 2048

$ more cluster_04/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host4

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 6
MaxSlices = 2048
</programlisting>
            </listitem>
            <listitem>
              <para>The <code>[ELASTIC]</code> section in the <code>cluster.ini</code> files above, enables the elastic
           cluster mode, where multiple <code>segments</code> and <code>stripes</code> as detailed in the standard
           Virtuoso documentation  <link linkend="ini_striping">database striping</link> .
           </para>
            </listitem>
            <listitem>
              <para>  The <code>Slices</code>

   parameter above should be set to the number of hardware threads on the
           CPUs running on. Thus in the example above where it is set to 6, this assumes all nodes are running on the
           same physical machine with 12 cores with hyper-threading enabled i.e. 24 threads, thus 6 per cluster node.</para>
            </listitem>
            <listitem>
              <para>The <code>MaxSlices</code> parameter above sets the the maximum number of physical slices in the cluster.
           </para>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconf7start">
          <title>Start Elastic Cluster</title>
          <para><emphasis>Step 4</emphasis>

  : Start Elastic Cluster</para>
          <orderedlist>
            <listitem>
              <para>Start the Elastic cluster using the standard
              <code>virtuoso-start.sh</code>
               script run from the home
           directory of the Virtuoso installation,which automatically detects the number for nodes to be started:
              <programlisting>

$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_01]
Starting Virtuoso instance in [cluster_02]
Starting Virtuoso instance in [cluster_03]
Starting Virtuoso instance in [cluster_04]
$
</programlisting>
            </para>
            </listitem>
            <listitem>
              <para>The default SQL port of the master node is 12201, as indicated in the virtuoso.ini file of the
           	cluster_01 directory, and can then be used for connecting to the newly created cluster and check its
           	status to ensure all nodes are online:
</para>
              <programlisting>
$ isql 12201
Connected to OpenLink Virtuoso
Driver: 07.10.3211 OpenLink Virtuoso ODBC Driver
OpenLink Interactive SQL (Virtuoso), version 0.9849b.
Type HELP; for help and EXIT; to exit.
SQL&gt; status('cluster_d');
REPORT
VARCHAR
_______________________________________________________________________________

Cluster: No samples, Please refresh

1 Rows. -- 22 msec.
SQL&gt; status('cluster_d');
REPORT
VARCHAR
_______________________________________________________________________________

Cluster 4 nodes, 2 s. 5 m/s 0 KB/s  5% cpu 0%  read 0% clw threads 1r 0w 0i buffers 349144 2 d 0 w 0 pfs
cl 1: 2 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 1r 0w 0i buffers 45391 2 d 0 w 0 pfs
cl 2: 0 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 0r 0w 0i buffers 43367 0 d 0 w 0 pfs
cl 3: 0 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 0r 0w 0i buffers 50129 0 d 0 w 0 pfs
cl 4: 0 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 0r 0w 0i buffers 39383 0 d 0 w 0 pfs

5 Rows. -- 22 msec.
SQL&gt;
SQL&gt; cl_ping(1,500, 1000);

Done. -- 7 msec.
SQL&gt; cl_ping(2,500, 1000);

Done. -- 52 msec.
SQL&gt; cl_ping(3,500, 1000);

Done. -- 60 msec.
SQL&gt; cl_ping(4,500, 1000);

Done. -- 51 msec.
SQL&gt;
</programlisting>
            </listitem>
            <listitem>
              <para>The
              <code>database.log</code>
               of the master node can be checked to verify the Elastic cluster mode
           has been enable on server startup, which will contain the entry

                <emphasis>PL LOG: Elastic cluster setup</emphasis>

              :
</para>
              <programlisting>
$ more cluster_01/database.log

                Thu Oct 09 2014
05:11:49 { Loading plugin 1: Type `plain', file `wikiv' in `../hosting'
05:11:49   WikiV version 0.6 from OpenLink Software
05:11:49   Support functions for WikiV collaboration tool
05:11:49   SUCCESS plugin 1: loaded from ../hosting/wikiv.so }
05:11:49 { Loading plugin 2: Type `plain', file `mediawiki' in `../hosting'
05:11:49   MediaWiki version 0.1 from OpenLink Software
05:11:49   Support functions for MediaWiki collaboration tool
05:11:49   SUCCESS plugin 2: loaded from ../hosting/mediawiki.so }
05:11:49 { Loading plugin 3: Type `plain', file `creolewiki' in `../hosting'
05:11:49   CreoleWiki version 0.1 from OpenLink Software
05:11:49   Support functions for CreoleWiki collaboration tool
05:11:49   SUCCESS plugin 3: loaded from ../hosting/creolewiki.so }
05:11:49 { Loading plugin 4: Type `plain', file `im' in `../hosting'
05:11:49   IM version 0.61 from OpenLink Software
05:11:49   Support functions for Image Magick 6.8.1
05:11:49   SUCCESS plugin 4: loaded from ../hosting/im.so }
05:11:49 { Loading plugin 5: Type `plain', file `wbxml2' in `../hosting'
05:11:49   WBXML2 version 0.9 from OpenLink Software
05:11:49   Support functions for WBXML2 0.9.2 Library
05:11:49   SUCCESS plugin 5: loaded from ../hosting/wbxml2.so }
05:11:49 { Loading plugin 6: Type `attach', file `libphp5.so' in `../hosting'
05:11:49   SUCCESS plugin 6: loaded from ../hosting/libphp5.so }
05:11:49 { Loading plugin 7: Type `Hosting', file `hosting_php.so' in `../hosting'
05:11:49   Hosting version 3208 from OpenLink Software
05:11:49   PHP engine version 5.3.21
05:11:49   SUCCESS plugin 7: loaded from ../hosting/hosting_php.so }
05:11:49 { Loading plugin 8: Type `plain', file `qrcode' in `../hosting'
05:11:49   QRcode version 0.1 from OpenLink Software
05:11:49   Support functions for ISO/IEC 18004:2006, using QR Code encoder (C) 2006 Kentaro Fukuchi &lt;fukichi@megaui.net&gt;
05:11:49   SUCCESS plugin 8: loaded from ../hosting/qrcode.so }
05:11:49 OpenLink Virtuoso Universal Server
05:11:49 Version 07.10.3211-pthreads for Linux as of Oct  6 2014
05:11:49 uses parts of OpenSSL, PCRE, Html Tidy
05:11:51 SQL Optimizer enabled (max 1000 layouts)
05:11:52 Compiler unit is timed at 0.000403 msec
05:12:03 Checkpoint started
05:12:04 Roll forward started
05:12:04 Roll forward complete
05:12:07 PL LOG: Elastic cluster setup
05:12:08 Checkpoint started
05:12:09 Checkpoint finished, log reused
05:12:11 Checkpoint started
05:12:12 Checkpoint finished, log reused
05:12:12 PL LOG: new clustered database:Init of RDF
05:12:23 Checkpoint started
05:12:25 Checkpoint finished, log reused
05:12:50 PL LOG: Installing Virtuoso Conductor version 1.00.8727 (DAV)
05:12:51 Checkpoint started
05:12:53 Checkpoint finished, log reused
05:13:23 Checkpoint started
05:13:25 Checkpoint finished, log reused
05:13:26 Checkpoint started
05:13:26 Checkpoint finished, log reused
05:13:28 HTTP/WebDAV server online at 8890
05:13:28 Server online at 12201 (pid 15211)
05:13:29 ZeroConfig registration CLUSTER (MASALA)
</programlisting>
            </listitem>
            <listitem>
              <para>The cluster node directories can also be checked, where the database slice/shard files i.e.

              <code>database.db.X, database.db.Y, database.db.Z</code>
               can be seen:
</para>
              <programlisting>
$ ls -ltr cluster_01
total 2322804
drwxr-xr-x. 2 virtuoso virtuoso      4096 Oct  9 04:26 backup
drwxr-xr-x. 2 virtuoso virtuoso      4096 Oct  9 04:26 logs
-rwxr-xr-x. 1 virtuoso virtuoso     70607 Oct  9 04:26 php.ini
lrwxrwxrwx. 1 virtuoso virtuoso        24 Oct  9 04:26 virtuoso -&gt; ..//bin/virtuoso-iodbc-t
-rw-r--r--. 1 virtuoso virtuoso         0 Oct  9 04:26 database.pxa
-rwxr-xr-x. 1 virtuoso virtuoso      6594 Oct  9 04:33 virtuoso.ini
-rw-r--r--. 1 virtuoso virtuoso       137 Oct  9 05:03 cluster.ini
lrwxrwxrwx. 1 virtuoso virtuoso        21 Oct  9 05:04 cluster.global.ini -&gt; ../cluster.global.ini
drwxr-xr-x. 2 virtuoso virtuoso    131072 Oct  9 05:37 dump
-rw-r--r--. 1 virtuoso virtuoso     32915 Oct  9 06:55 database.2pc
-rw-r--r--. 1 virtuoso virtuoso        25 Oct 10 02:24 database.map
-rw-r--r--. 1 virtuoso virtuoso  56623104 Oct 10 03:23 database-temp.db
-rw-r--r--. 1 virtuoso virtuoso 824180736 Oct 10 05:10 database.db.16
-rw-r--r--. 1 virtuoso virtuoso 723517440 Oct 10 05:10 database.db.8
-rw-r--r--. 1 virtuoso virtuoso 740294656 Oct 10 05:10 database.db.0
-rw-r--r--. 1 virtuoso virtuoso  33554432 Oct 10 05:10 database.db
-rw-r--r--. 1 virtuoso virtuoso         0 Oct 10 05:10 database.trx
-rw-r--r--. 1 virtuoso virtuoso     34234 Oct 10 05:10 database.log
$
</programlisting>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconf7split">
          <title>Splitting Cluster nodes across different machines</title>
          <para><emphasis>Step 5</emphasis>

  : Splitting Cluster nodes across different machines</para>
          <orderedlist>
            <listitem>
              <para>To split the node across across physical machines for better scale out performance, scalability
           	and growth, simply perform a parallel Virtuoso installation on the additional physical machines and move
           	the cluster nodes required to the designated machine. Example, for the default 4 node cluster to be split
           	across two identical machines it would make sense to split 2 nodes across each machine, thus you would
           	move say the cluster_03 and cluster_04 directory nodes to the new machine (removing them from the original).
           	The
              <code>cluster.global.ini</code>
               file on each node would then need to be updated to set the HostXX
           	parameters to point to the new locations for nodes 03 and 04:
</para>
              <programlisting>
$ more cluster.global.ini

[Cluster]
Threads             = 300
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 100
RDFLoadBytes        = 52428800
Host1               = hostname1:22201
Host2               = hostname1:22202
Host3               = hostname2:22203
Host4               = hostname2:22204

Machine 1 (hostname1)

$ more cluster_01/cluster.ini
[Cluster]
Master              = Host1
ThisHost            = Host1

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 12
MaxSlices = 2048

$ more cluster_02/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host2

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 12
MaxSlices = 2048

Machine 2 (hostname2)

$ more cluster_03/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host3

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 12
MaxSlices = 2048

$ more cluster_04/cluster.ini

[Cluster]
Master              = Host1
ThisHost            = Host4

[ELASTIC]
Segment1 = 4G, database.db = q1
Slices = 12
MaxSlices = 2048
</programlisting>
            </listitem>
            <listitem>
              <para>Note assuming the same number of CPU threads on each machine i.e. 24, then the
              <code>Slices</code>

           param can be doubled to 12 for each node in
              <code>cluster.ini</code>
              , as above.
           </para>
            </listitem>
            <listitem>
              <para>The cluster nodes can then be started on each machine, with 2 nodes being started on each in this
           	case to form the cluster:
</para>
              <programlisting>
Machine 1 (hostname1)

$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_01]
Starting Virtuoso instance in [cluster_02]
$

Machine 2 (hostname2)

$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_03]
Starting Virtuoso instance in [cluster_04]
$
</programlisting>
            </listitem>
            <listitem>
              <para>To stop the cluster use the standard
              <code>virtuoso-stop.sh</code>
               script on each machine, which
           automatically detects the number for nodes to be stopped:
</para>
              <programlisting>
Machine 1 (hostname1)

$ virtuoso-stop.sh
Stopping Virtuoso instance in [cluster_01]
Stopping Virtuoso instance in [cluster_02]

Machine 2 (hostname2)

$ virtuoso-stop.sh
Stopping Virtuoso instance in [cluster_03]
Stopping Virtuoso instance in [cluster_04]
$
</programlisting>
            </listitem>
            <listitem>
              <para>The
              <code>cl_exec('shutdown')</code>
               command can also be run from
              <code>isql</code>
               on any node
           of the cluster to shutdown all nodes at once:
</para>
              <programlisting>
$ isql 12201
Connected to OpenLink Virtuoso
Driver: 07.10.3211 OpenLink Virtuoso ODBC Driver
OpenLink Interactive SQL (Virtuoso), version 0.9849b.
Type HELP; for help and EXIT; to exit.
SQL&gt; cl_exec ('checkpoint');

Done. -- 2487 msec.
SQL&gt; cl_exec ('shutdown');

*** Error 08S01: VD CL065: Lost connection to server
at line 2 of Top-Level:
cl_exec ('shutdown')
$
</programlisting>
            </listitem>
          </orderedlist>
        </section>
      </section>
      <section xml:id="clusterstcnfsetup">
        <title>Virtuoso default Cluster Installation and Configuration</title>
        <para>These sections apply to Virtuoso as of version 6.x and higher.</para>
        <orderedlist>
          <listitem>
            <para>Run the Virtuoso 6 Unix ( MacOsX) installer to perform a default installation:
</para>
            <programlisting>
$ tar xvf virtuoso-universal-server-6.1.tar
x install.sh
x universal-server.taz
$ sh install.sh

- Extracting Virtuoso Universal Server v6.1

- Creating default environment settings

- Creating default database settings
Configuring: database
Creating directory $VIRTUOSO_HOME/database/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/database
Installing new php.ini in $VIRTUOSO_HOME/database
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t

- Registering ODBC drivers

- Registering .NET provider for Mono

- Installing VAD packages in database (this can take some time)

- Checking where license should be stored

- Starting OpenLink License Manager

- Checking for initial Virtuoso license

- Starting Virtuoso server instance

- Finalizing installation

This concludes the first part of the installation.

- Attempting to start browser

Please start a browser manually and open the following URL to finalize
the installation process:

     http://cname:8890/install/

Installation completed
$
</programlisting>
          </listitem>
          <listitem>
            <para>Determine how many nodes you want to start the cluster with: 2, 3, 4, 5, 6, 7, 8 etc. </para>
            <para>
              <emphasis>Note</emphasis>
            </para>
            <para>:
  the number of the nodes should be &gt;= 2</para>
          </listitem>
          <listitem>
            <para>Setup Virtuoso environment and stop default database:
</para>
            <programlisting>
$ . ./virtuoso-environment.sh
$ virtuoso-stop.sh
Shutting down Virtuoso instance in [database]
$
</programlisting>
          </listitem>
          <listitem>
            <para>Optionally remove the default database such that is does not get started with the cluster:
</para>
            <programlisting>
rm  -rf database
</programlisting>
          </listitem>
          <listitem>
            <para>Full list of supported options for the script </para>
            <para>
              <emphasis>"virtuoso-mkcluster.sh"</emphasis>
            </para>
            <para>:
</para>
            <programlisting>
  -cluster-size       Number of nodes in cluster
  -cluster-node       Node number in a cluster
  -cluster-port       Base portnumer for cluster
  -cluster-ipc-port   Base portnumer for cluster IPC
  -cluster_size       Size of the cluster
  -virtuoso_home      Virtuoso home path
</programlisting>
          </listitem>
          <listitem>
            <para>Run the virtuoso-mkcluster.sh script to create the cluster, note the default number of
  	nodes is 4, but this can be changed by setting the environment variable CLUSTER_SIZE to the
  	required number of nodes to be created before running the script:
</para>
            <programlisting>
$ virtuoso-mkcluster.sh
Configuring cluster node: 1/4

Creating directory $VIRTUOSO_HOME/cluster_01
Creating directory $VIRTUOSO_HOME/cluster_01/backup
Creating directory $VIRTUOSO_HOME/cluster_01/logs
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_01
Installing new php.ini in $VIRTUOSO_HOME/cluster_01
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 2/4

Creating directory $VIRTUOSO_HOME/cluster_02
Creating directory $VIRTUOSO_HOME/cluster_02/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_02
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 3/4

Creating directory $VIRTUOSO_HOME/cluster_03
Creating directory $VIRTUOSO_HOME/cluster_03/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_03
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
Configuring cluster node: 4/4

Creating directory $VIRTUOSO_HOME/cluster_04
Creating directory $VIRTUOSO_HOME/cluster_04/backup
Installing new virtuoso.ini in $VIRTUOSO_HOME/cluster_04
Removing unneeded sections from virtuoso.ini
Creating symlink to $VIRTUOSO_HOME/bin/virtuoso-iodbc-t
$
</programlisting>
          </listitem>
          <listitem>
            <para>For each of the cluster_XX directories created edit the cluster.ini file and set the
  	HostXX parameter to the hostname:portno to be used by the cluster nodes for internal communication.
</para>
            <programlisting>
$ more cluster_01/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host1
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname:22201
Host2               = hostname:22202
Host3               = hostname:22203
Host4               = hostname:22204
MaxHosts            = 5
$ more cluster_02/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host2
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname:22201
Host2               = hostname:22202
Host3               = hostname:22203
Host4               = hostname:22204
MaxHosts            = 5
$ more cluster_03/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host3
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname:22201
Host2               = hostname:22202
Host3               = hostname:22203
Host4               = hostname:22204
MaxHosts            = 5
$ more cluster_04/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host4
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname:22201
Host2               = hostname:22202
Host3               = hostname:22203
Host4               = hostname:22204
MaxHosts            = 5
$
</programlisting>
          </listitem>
          <listitem>
            <para>Start the cluster using the standard virtuoso-start.sh script which automatically detects
  	the number for nodes to be started:
</para>
            <programlisting>
$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_01]
Starting Virtuoso instance in [cluster_02]
Starting Virtuoso instance in [cluster_03]
Starting Virtuoso instance in [cluster_04]
$
</programlisting>
          </listitem>
          <listitem>
            <para>The default SQL port of the master node is 12201, as indicated in the virtuoso.ini file
  	of the cluster_01 directory, and can then be used for connecting to the newly created cluster
  	and check its status to ensure all nodes are online:
</para>
            <programlisting>
$ isql 12201
Connected to OpenLink Virtuoso
Driver: 06.01.3127 OpenLink Virtuoso ODBC Driver
OpenLink Interactive SQL (Virtuoso), version 0.9849b.
Type HELP; for help and EXIT; to exit.
SQL&gt; status ('cluster');
REPORT
VARCHAR
_______________________________________________________________________________

Cluster 4 nodes, 4 s. 1 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 1r 0w 0i buffers 2981 0 d 0 w 0 pfs

1 Rows. -- 7 msec.
SQL&gt; cl_ping(1,500, 1000);

Done. -- 7 msec.
SQL&gt; cl_ping(2,500, 1000);

Done. -- 52 msec.
SQL&gt; cl_ping(3,500, 1000);

Done. -- 60 msec.
SQL&gt; cl_ping(4,500, 1000);

Done. -- 51 msec.
SQL&gt;
</programlisting>
          </listitem>
          <listitem>
            <para>To split the node across across physical machines for better performance and scalability,
  	simply perform a parallel Virtuoso installation on the additional physical machines and move the
  	cluster nodes required to the designated machine. Example, for the default 4 node cluster to be
  	split across two identical machines it would make sense to split 2 nodes across each machine,
  	thus you would move say the cluster_03 and cluster_04 directory nodes to the new machine (removing
  	them from the original). The cluster.ini file on each node would then need to be updated to set the
  	HostXX parameters to point to the new locations for nodes 03 and 04:
</para>
            <programlisting>
Machine 1 (hostname1)

$ more cluster_01/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host1
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname1:22201
Host2               = hostname1:22202
Host3               = hostname2:22203
Host4               = hostname2:22204
MaxHosts            = 5
$ more cluster_02/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host2
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname1:22201
Host2               = hostname1:22202
Host3               = hostname2:22203
Host4               = hostname2:22204
MaxHosts            = 5
$

Machine 2 (hostname2)

$ more cluster_03/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host3
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname1:22201
Host2               = hostname1:22202
Host3               = hostname2:22203
Host4               = hostname2:22204
MaxHosts            = 5
$ more cluster_04/cluster.ini
[Cluster]
Threads             = 20
Master              = Host1
ThisHost            = Host4
ReqBatchSize        = 10000
BatchesPerRPC       = 4
BatchBufferBytes    = 20000
LocalOnly           = 2
MaxKeepAlivesMissed = 1000
Host1               = hostname1:22201
Host2               = hostname1:22202
Host3               = hostname2:22203
Host4               = hostname2:22204
MaxHosts            = 5
$
</programlisting>
          </listitem>
          <listitem>
            <para>The nodes can then be started on each machine, with 2 nodes being started on each in this
  	case to form the cluster:
</para>
            <programlisting>

Machine 1 (hostname1)

$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_01]
Starting Virtuoso instance in [cluster_02]
$

Machine 2 (hostname2)

$ virtuoso-start.sh
Starting Virtuoso instance in [cluster_03]
Starting Virtuoso instance in [cluster_04]
$
</programlisting>
          </listitem>
          <listitem>
            <para>To stop the cluster use the standard virtuoso-stop.sh script which automatically detects the number for nodes to be stopped:
</para>
            <programlisting>
$ virtuoso-stop.sh
Stopping Virtuoso instance in [cluster_01]
Stopping Virtuoso instance in [cluster_02]
Stopping Virtuoso instance in [cluster_03]
Stopping Virtuoso instance in [cluster_04]
$
</programlisting>
          </listitem>
        </orderedlist>
      </section>
      <section xml:id="clusterstcnfbackuprestore">
        <title>Backup and Restore</title>
        <para>
          <emphasis>Backup</emphasis>
        </para>
        <para>There are 2 ways to backup Virtuoso Cluster DB:</para>
        <orderedlist>
          <listitem>
            <para>Backup every node using
            <link linkend="fn_backup_online"><function>back_online</function></link>
              <emphasis>For example, for every cluster in its backup folder execute:</emphasis>
            </para>
            <programlisting>
SQL&gt; backup_online('dbp',10000000,0,vector('backup'));

Done. -- 272 msec.
</programlisting>
          </listitem>
          <listitem>
            <para>Make backup of all nodes at once using
            <link linkend="fn_cl_exec">cl_exec()</link>
            . For example, execute:
</para>
            <programlisting>
SQL&gt; cl_exec ('backup_online (''dbp'', 10000000, 0,  vector (''backup''))');

Done. -- 573 msec.
</programlisting>
          </listitem>
        </orderedlist>
        <para>
          <emphasis>Restore from Backup</emphasis>
        </para>
        <para>After backup is done for every node in its backup folder, in order to restore,
execute from the ..\bin folder the virtuoso-restore.sh:</para>
        <programlisting>
# . ./virtuoso-restore.sh all dbp
</programlisting>
        <para>where the second parameter is the restore prefix.</para>
      </section>
      <section xml:id="clusterstcnfconfig">
        <title>Cluster Configuration</title>
        <orderedlist>
          <listitem>
            <para>Set "FAST_START=1" in the viruoso-start.sh file and then run:
</para>
            <programlisting>
viruoso-start.sh
</programlisting>
            <para>or</para>
          </listitem>
          <listitem>
            <para>Execute the following line:
</para>
            <programlisting>
# /etc/init.d/virtuoso start
Starting OpenLink Virtuoso:                                [  OK  ]
</programlisting>
          </listitem>
          <listitem>
            <para>In order to check the nodes, connect to port 12201 using the ISQL tool:
</para>
            <programlisting>
isql 12201
</programlisting>
          </listitem>
          <listitem>
            <para>To check the cluster status, execute the following command:
</para>
            <programlisting>
SQL&gt; status('cluster');
REPORT
VARCHAR
_______________________________________________________________________________

Cluster 4 nodes, 293 s. 0 m/s 0 KB/s  0% cpu 0%  read 0% clw threads 1r 0w 0i buffers 1781 0 d 0 w 0 pfs

1 Rows. -- 4 msec.
</programlisting>
          </listitem>
        </orderedlist>
      </section>
      <!-- ======================================== -->

      <section xml:id="clusterstcnfconfnodesconfig">
        <title>HTTP Service Configuration on Subordinate Nodes of a Virtuoso Cluster</title>
        <para>This section applies to Virtuoso as of version 6.x and higher.</para>
        <section xml:id="clusterstcnfconfnodesconfigwhat">
          <title>What</title>
          <para>This documentation details how to configure the Subordinate (also called Slave) Nodes of a Virtuoso Elastic Cluster
         	to service HTTP clients.</para>
        </section>
        <section xml:id="clusterstcnfconfnodesconfigwhy">
          <title>Why</title>
          <para>By default, only the Primary (also called Master) instance of a Virtuoso Elastic Cluster is configured to provide
         	HTTP services.</para>
          <para>The Subordinate (also called Slave) nodes of the cluster may also be configured to provide HTTP services,
         	enabling load balancing by spreading HTTP requests across the cluster's nodes.</para>
        </section>
        <section xml:id="clusterstcnfconfnodesconfighow">
          <title>How</title>
          <para>This documentation details the steps for the installation and configuration of a Virtuoso Elastic Cluster on Unix:</para>
          <orderedlist>
            <listitem>
              <para>
                <emphasis>Step 1</emphasis>
              </para>
              <para>: Set up each instance as a HTTP Server;</para>
            </listitem>
            <listitem>
              <para>
                <emphasis>Step 2</emphasis>
              </para>
              <para>: Install and configure HTTP services on each instance;</para>
            </listitem>
            <listitem>
              <para>
                <emphasis>Step 3</emphasis>
              </para>
              <para>: Configure load balancing.</para>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconfnodesconfiginst">
          <title>Set up each instance as an HTTP Server</title>
          <para><emphasis>Step 1</emphasis>

  : Set up each instance as a HTTP Server</para>
          <para>Each node can be configured to provide HTTP services as follows:</para>
          <orderedlist>
            <listitem>
              <para>Copy the
              <code>[HTTP Server]</code>
               section from the Primary instance's configuration file (by
           default,
              <code>virtuoso.ini</code>
              ) to the configuration file of each Subordinate instance:
</para>
              <programlisting>
[HTTPServer]
ServerPort                  = 8890
ServerRoot                  = ../vsp
DavRoot                     = DAV
EnabledDavVSP               = 0
HTTPProxyEnabled            = 0
TempASPXDir                 = 0
DefaultMailServer           = localhost:25
MaxClientConnections        = 5
MaxKeepAlives               = 10
KeepAliveTimeout            = 10
MaxCachedProxyConnections   = 10
ProxyConnectionCacheTimeout = 15
HTTPThreadSize              = 280000
HttpPrintWarningsInOutput   = 0
Charset                     = UTF-8
;HTTPLogFile                 = logs/http.log
MaintenancePage             = atomic.html
EnabledGzipContent          = 1
</programlisting>
            </listitem>
            <listitem>
              <para>Edit the
              <code>ServerPort</code>
               parameter to make it unique on the machine hosting this instance;
           i.e., if a subordinate instance is running on same physical node as the primary instance, then the subordinate's
           HTTP port must to be changed (from 8890, for instance) to a unique port (e.g., 8891).</para>
            </listitem>
            <listitem>
              <para>Install the Virtuoso Conductor to enable HTTP Administration of the instance being configured. Note:
           if the subordinate instance is not on the same machine as the primary instance, then the vad directory may also
           need to be copied from the primary instance to the subordinate instance.:
</para>
              <programlisting>
SQL&gt; vad_install ('../vad/conductor_dav.vad', 0);
SQL_STATE  SQL_MESSAGE
VARCHAR  VARCHAR
_______________________________________________________________________________

00000    No errors detected
00000    Installation of "Virtuoso Conductor" is complete.
00000    Now making a final checkpoint.
00000    Final checkpoint is made.
00000    SUCCESS

6 Rows. -- 10263 msec.
SQL&gt;
</programlisting>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconfnodesconfigservice">
          <title>Install and configure HTTP services on each instance</title>
          <para><emphasis>Step 2</emphasis>

  : Install and configure HTTP services on each instance</para>
          <para>Any HTTP services required on the subordinate instance will need to specifically installed or configured on
         	that physical node. For example, the Virtuoso default SPARQL endpoint (<code>/sparql</code>

  ) may be configured by:</para>
          <orderedlist>
            <listitem>
              <para>Log in into the Virtuoso Conductor http://hostname:port/conductor :
             </para>
              <figure xml:id="clst1" floatstyle="1">
                <title>Configure SPARQL Endpoint: log in</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster1.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>Go the the Web Application Server -&gt; Virtual Domains &amp; Directories tab:
             </para>
              <figure xml:id="clst2" floatstyle="1">
                <title>Configure SPARQL Endpoint: Virtual Domains and Directories</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster2.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>Select the New Directory Action for the Default Web Site HTTP host:
             </para>
              <figure xml:id="clst3" floatstyle="1">
                <title>Configure SPARQL Endpoint: new directory</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster3.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>Select the Type radio button and SPARQL access point item from the drop down list box:
             </para>
              <figure xml:id="clst4" floatstyle="1">
                <title>Configure SPARQL Endpoint: set type SPARQL</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster4.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>Click "Next".</para>
            </listitem>
            <listitem>
              <para>Enter /sparql as the Path param in the Virtual Directory Information section and click Save Changes:
             </para>
              <figure xml:id="clst5" floatstyle="1">
                <title>Configure SPARQL Endpoint: set /sparql virtual directory</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster5.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>The SPARQL endpoint will not be accessible on http://hostname:port/sparql the the newly configured slave nodes:
             </para>
              <figure xml:id="clst6" floatstyle="1">
                <title>Configure SPARQL Endpoint: SPARQL Endpoint</title>
                <mediaobject>
                  <imageobject>
                    <imagedata fileref="ui/cluster6.jpg"/>
                  </imageobject>
                </mediaobject>
              </figure>
            </listitem>
            <listitem>
              <para>Further details on SPARQL endpoint configuration can be found in

              <link linkend="rdfsupportedprotocolendpoint">Service Endpoint</link>
               documentation section.</para>
            </listitem>
            <listitem>
              <para>Typical Virtuoso server log output from a slave node when started, showing the HTTP server running on
           	port 8890, being:
</para>
              <programlisting>
20:12:49 OpenLink Virtuoso Universal Server
20:12:49 Version 07.10.3209-pthreads for Linux as of Apr 26 2014
20:12:49 uses parts of OpenSSL, PCRE, Html Tidy
20:12:49 Registered to OpenLink Virtuoso (Internal Use)
20:12:49 Personal Edition license for 500 connections
20:12:49 Issued by OpenLink Software
20:12:49 This license will expire on Sun May 17 06:18:35 2015 GMT
20:12:49 Enabled Cluster Extension
20:12:49 Enabled Column Store Extension
20:12:57 Database version 3126
20:12:57 SQL Optimizer enabled (max 1000 layouts)
20:12:58 Compiler unit is timed at 0.000208 msec
20:12:58 Roll forward started
20:12:58 Roll forward complete
20:12:59 Checkpoint started
20:12:59 Checkpoint finished, log reused
20:12:59 HTTP/WebDAV server online at 8890
20:12:59 Server online at 12202 (pid 15969)
</programlisting>
            </listitem>
          </orderedlist>
        </section>
        <section xml:id="clusterstcnfconfnodesconfigload">
          <title>Configure load balancing</title>
          <para><emphasis>Step 3</emphasis>

  : Configure load balancing</para>
          <para>A reverse-proxy service (like Nginx or Apache) can then be configured such that requests are proxied across
         	as any or all nodes of the cluster, to provide the desired load balancing.</para>
        </section>
        <section xml:id="clusterstcnfconfnodesconfigaddinf">
          <title>Additional Information</title>
          <itemizedlist mark="bullet">
            <listitem>
              <para>Only the Primary Node of an Elastic Cluster may be configured as a Publisher for Virtuoso Replication
           	Cluster purposes.</para>
            </listitem>
            <listitem>
              <para>The
              <link xlink:href="http://virtuoso.openlinksw.com/whitepapers/LOD2_D2.1.5_LOD_Cloud_Hosted_On_The_LOD2_Knowledge_Store_Cluster_500B_Triples.pdf">Virtuoso 500 billion triple Berlin SPARQL Benchmark (BSBM) dataset</link>

           runs were performed on a 24-node Elastic Cluster. Each node was configured to provide HTTP services and a SPARQL
           endpoint, and the query load was spread over the entire cluster.</para>
            </listitem>
          </itemizedlist>
        </section>
      </section>
      <section xml:id="clusterstcnftrsh">
        <title>Troubleshooting Tips</title>
        <para>If an operation seems to hang, see the output of:</para>
        <programlisting>
status ()
</programlisting>
        <para>Check for the presence of the following conditions:</para>
        <itemizedlist>
          <listitem>
            <para>The cluster line shows 0% CPU, no message traffic and an unchanging number of buffers
wired, this is probably a bug. To reset, restart the cluster or the offending process if found.
Restart is done by executing:
</para>
            <programlisting>
raw_exit ();
</programlisting>
            <para>over an SQL connection to the process in question.</para>
          </listitem>
          <listitem>
            <para>The cluster line shows many threads waiting compared to total threads. If CPU is 0 and
this does not change there could be a transaction that holds locks indefinitely. To clear, execute:
</para>
            <programlisting>
txn_killall (1);
</programlisting>
            <para>Do this at a node that has local threads waiting. This is seen in the Lock Status
paragraph of status ('') when connected to the node in question.</para>
          </listitem>
          <listitem>
            <para>The cluster line shows a changing number in the pfs field. The system is swapping
and slowed down.</para>
          </listitem>
          <listitem>
            <para>If the status () itself hangs, try another process of the cluster. See that there is
no temporary atomic activity like a long checkpoint. If the situation persists there is a bug.
The checkpoint can be seen by the presence of the </para>
            <para>
              <emphasis>checkpinmt_in_progress</emphasis>
            </para>
            <para> file
in each server's working directory.</para>
          </listitem>
          <listitem>
            <para>To check the integrity of database files, do:
</para>
            <programlisting>
cl_exec ('backup ''/dev/null''');
</programlisting>
            <para>If this returns, the databases are OK. If one is found to be corrupt the corresponding server exits.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section>
        <title>See Also:</title>
        <tip>
          <para>
            <link linkend="clusteroperation">Setting up and operating Virtuoso on a cluster.</link>
          </para>
          <para>
            Virtuoso Cluster Programming
          </para>
          <para>
            <link linkend="fault">Virtuoso Cluster Fault Tolerance.</link>
          </para>
        </tip>
      </section>
    </section>
