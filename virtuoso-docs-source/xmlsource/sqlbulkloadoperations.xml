<?xml version="1.0" encoding="UTF-8"?>
<section xmlns:xlink="http://www.w3.org/1999/xlink" xmlns="http://docbook.org/ns/docbook" xml:id="sqlbulkloadoperations">
      <title>SQL Bulk Load, ELT, File Tables and Zero Load Operations</title>
      <section xml:id="sqlbulkloadoperationsftable">
        <title>File Tables</title>
        <para>Virtuoso supports mapping files of comma separated values as tables from version 7.00.3206. This makes
    	bulk load of relational data straightforward and very fast. This also makes it possible to query data in
    	files without any loading into the database. The latter is specially convenient if data is queried just
    	once, where loading and subsequently dropping the data would be needless overhead. Also arbitrary ETL
    	transformations can be expressed in SQL, reading from a file table and inserting into a database table.
    	This includes the whole range of SQL functionality, including intermediate aggregations and the like.
    </para>
        <para>This feature deprecates the CSV load functions in previous versions, e.g. csv_load.
    </para>
        <para>Queries can freely mix file tables and regular tables. Joins with file tables make hash join plans
    	where the smaller file is copied into a memory resident hash table. For very large hash tables, partitioned
    	hash join can be used, thus arbitrary joining between files is possible, fast and convenient.
    </para>
        <para>A file table is declared with a regular create table statement. If the table definition includes a
    	primary key, the file is assumed to be sorted as declared. In most cases a primary key should not be
    	declared when creating a file table. After the create table statement, the procedure fs_set_file declares
    	that the table is in fact a file. In a cluster setting the file should be visible to all the server
    	processes via a shared file system.</para>
        <para>For example, to bulk load the TPC-H part table:</para>
        <programlisting>
CREATE TABLE PART_F (
    P_PARTKEY     INTEGER NOT NULL,
    P_NAME        VARCHAR(55) NOT NULL,
    P_MFGR        CHAR(25) NOT NULL,
    P_BRAND       CHAR(10) NOT NULL,
    P_TYPE        VARCHAR(25) NOT NULL,
    P_SIZE        INTEGER NOT NULL,
    P_CONTAINER   CHAR(10) NOT NULL,
    P_RETAILPRICE double precision NOT NULL,
    P_COMMENT     VARCHAR(23) NOT NULL    );

ft_set_file ('PART_F', 'src/part.tbl', delimiter =&gt; '|');

-- turn off logging and set insert mode to auto committing, non-transactional
log_enable (2);

-- read the file and insert into the previously create part table
insert into part select * from part_f;

</programlisting>
        <para>The ft_set_file procedure takes the table name as first argument and a file system path relative to
    	the server's working directory as the second argument. Optional arguments allow specifying a delimiter,
    	newline character and escape character.
    </para>
        <programlisting>
	create procedure ft_set_file (in tb varchar, in fname varchar, in delimiter varchar, in newline varchar :=  '\n', in esc varchar := null)
</programlisting>
        <para>Creating a file table requires dba group privileges and the file is subject to the file system access
    	limitations that apply to file_to_string and other file system access functions.</para>
        <para>The newline and escape characters need to be single character strings. A newline or escape character
    	following the escape character is added to the parsed input without its special interpretation.</para>
        <para>Each column in the CSV file is expected to end with the delimiter character. A field of zero length
    	is considered a SQL NULL value, i.e. if two delimiters are adjacent or if a line begins with the delimiter,
    	the field is considered NULL. The text in the field is parsed according to the data type declared for the
    	column whose position in the create table corresponds to the field position on the line. The parsing is
    	as by the SQL cast function from a varchar value. If the cast fails the line is silently ignored.
    </para>
        <para>When a table is declared as a file table, the file is sampled and statistics are written into the
    	sys_col_stat table. In this way the system is capable of making correct query plans involving joins of
    	file tables. For this reason, the file should exist and have the relevant content when the file table
    	is created.
    </para>
        <para>A file table will be read in parallel on multiple threads if a normal table would be read in
    	parallel in the same situation. In a single server, the ThreadsPerQuery (enable_qp in sys_stat)
    	setting controls the number of threads used. In a cluster setting each elastic slice corresponds
    	to a fraction of the file. This is why the file is expected to be visible from all the servers.
    </para>
        <para>If a table had previous content and subsequently was declared a file table, the previous content
    	will no longer be visible. The file table may cease to be a file table by the dba deleting the
    	corresponding row from sys_file_table and calling __ddl_changed on the file table. At this point
    	the previous, databases resident content of the table will be visible again. However the statistics
    	gathered from the file will still shadow the statistics of the table.
    </para>
        <para>It is often the case that many CSV files have the same structure. For these cases it is sufficient
    	to create a single table and attach it to one of the files with ft_set_file. This file will provide
    	the statistics. In order to read a different file, one can use the FROM clause in the TABLE OPTION
    	clause of in the FROM clause of a select statement, as follows:
   	</para>
        <programlisting>
-- get the count of well formed part rows from the part.tbl file
SELECT COUNT (*)
  FROM part_f;

-- do the same from src/part.tbl.2
SELECT COUNT (*)
  FROM part_f TABLE OPTION (FROM 'src/part.tbl.2');
</programlisting>
        <para>The value of the FROM option can be an arbitrary expression, however it must be independent of
    	values bound by the containing statement. It may depend on variables bound in an enclosing stored
    	procedure or from parameters but it cannot join to a column of another table in the same select
    	statement.
    </para>
        <para>Arbitrary search conditions can be applied to file tables, for example:</para>
        <programlisting>
SELECT p_name, p_size, p_partkey
  FROM part_f
 WHERE p_name LIKE '%green%';
</programlisting>
        <para>Joins are possible, for example:</para>
        <programlisting>
SELECT SUM (ps_availqty)
  FROM partsupp, part_f
 WHERE ps_partkey = p_partkey
   AND p_name LIKE '%green%';
</programlisting>
        <para>This would join the partsupp table to the part file.</para>
        <para>File tables are not cached by the DBMS, they are read from the OS every time they are needed. A single
    	query will not read the same file multiple times, unless in the case of a partitioned (multtipass) hash
    	join. Nested loop joins are not applied to file tables. File tables cannot be updated and no transaction
    	control applies to them. Select permissions can be granted as for any other table.
    </para>
        <para> 
  </para>
      </section>
      <section xml:id="sqlbulkloadoperationsftableparallel">
        <title>Parallel Insert With File Tables and Transactions</title>
        <para>A file table is copied into a database resident table with an INSERT... SELECT statement. Such a
    	statement executes in parallel if the session is in auto commit mode, i.e. log_enable (2) or
    	log_enable (3) has been previously executed on the session.
    </para>
        <para>A file can be loaded inside a transaction if the connection is not in auto commit log_enable (2) or 3.
    	This will be multithreaded if enable_mt_txn is 1 in the [Flags] section of the ini file or
    	__dbf_set ('enable_mt_txn', 1) has been executed previously. The setting is global. Defaults vary according
    	to server version. Use sys_stat ('enable_nt_txn') to read the value of the setting.
    </para>
        <para>For long files the transaction is liable to run out of rollback space. File table operations as such
    	do not affect the transaction context. Explicit commits may be interspersed in a select statement from a
    	file or other tables.
    </para>
        <para>For example, the history keeping dimension updates from TPC DS can be implemented as follows. The item
    	table is a history keeping dimension that has an index on the i_item_id business key and has a primary key
    	of item surrogate key, with a new value for each version of the item record. The item record has a start
    	and end date (i_rec_start_date, i_rec_end_date) to mark the period of validity of the information. A null
    	value in item_rec_end_date marks the currently applicable record. When the item data is updated, the, a
    	new item is inserted and the previously current item record has its end date set to the current date.
    	These operations must occur atomically. Otherwise the implementation may choose whether to update many
    	item records in the same transaction.
    </para>
        <para>In the below listing most columns have been left out for brevity:</para>
        <programlisting>
CREATE PROCEDURE item_update (in i_id varchar,...)
{
  vectored;

  UPDATE item
     SET i_rec_end_date = curdate
   WHERE i_id = i_id
     AND i_rec_end_date IS NULL;

  INSERT INTO item (i_ietm_sk, i_item_id, i_rec_end_date,...)
            VALUES (sequence_next ('item_sk_seq'), i_id, NULL,... )
  not vectored { COMMIT WORK};
}

SELECT COUNT (item_update (i_item_id,....)
  FROM item_f....;
</programlisting>
        <para>The select statement call the item_update procedure on a vector of item ids and other columns.
    	The procedure marks the expired record and inserts the new record, assigning new surrogate keys from a
    	sequence. After each batch it performs one commit. The next batch of items are updated in a separate
    	transaction.
    </para>
        <para>This should be run on a single thread. In a multithreaded transaction the threads may not issue
    	individual commits. The code could be multithreaded by leaving out the commit from the stored procedure.
    	Then the commit would have to be after the completion of the select statement.
    </para>
        <para>The following isql script bulk loads a whole TPC-H database. We leave out the create tables and
    	ft_set_files for brevity, they are all as in the part example above.
    </para>
        <programlisting>
log_enable (2); INSERT INTO lineitem SELECT * FROM lineitem_f &amp;
log_enable (2); INSERT INTO orders   SELECT * FROM orders_f &amp;
log_enable (2); INSERT INTO customer SELECT * FROM customer_f &amp;
log_enable (2); INSERT INTO part     SELECT * FROM part_f &amp;
log_enable (2); INSERT INTO partsupp SELECT * FROM partsupp_f &amp;
log_enable (2); INSERT INTO supplier SELECT * FROM supplier_f &amp;
log_enable (2); INSERT INTO nation   SELECT * FROM nation_f &amp;
log_enable (2); INSERT INTO region   SELECT * FROM region_f &amp;

wait_for_children;
checkpoint;
</programlisting>
        <para>A multithreaded, non-logged, non-transactional insert is started for each table-file pair as a
    	background task. The wait_for_children isql command waits for all the background tasks to complete.
    	The checkpoint statement makes the state durable. Killing the server in before the checkpoint would
    	result in the server starting in a state with none of the effects of the bulk load present, since the
    	log_enable(2) turns off logging. The database is online during the bulk load and the progress may be
    	followed by periodically counting the tables, for example.
   </para>
      </section>
    </section>
